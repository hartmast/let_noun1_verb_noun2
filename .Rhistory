library(tidyverse)
d <- readLines("https___direct.mit.edu_books_oa-edited-volume_5244_The-Open-Handbook-of-Linguistic-Data-Management.html")
d
grep("chapter-pdf",d, value=T)
d1 <- grep("chapter-pdf",d, value=T)
gsub("<.*?>", "",d1)
gsub(".*?<href=\"", "",d1)
gsub(".*<href=\"", "",d1)
gsub(".*href=\"", "",d1)
d2 <- gsub(".*href=\"", "",d1)
gsub("view-source:", "", d2)
gsub("\".*", "", gsub("view-source:", "", d2))
d3 <- gsub("\".*", "", gsub("view-source:", "", d2))
download.file(url = d3[1], destfile = "001.pdf")
library(stringi)
stri_pad_left(2, 5, 0)
stri_pad_left(2, 3, 0)
stri_pad_left(12, 3, 0)
paste0(stri_pad_left(12, 3, 0), ".pdf")
Sys.sleep(sample(0:10), 1)
Sys.sleep(sample(0:10), n = 1)
Sys.sleep(sample(0:10, 1))
for(i in 2:length(d3)) {
download.file(url = d3[i], destfile = paste0(stri_pad_left(i, 3, 0), ".pdf"))
Sys.sleep(sample(0:10, 1))
print(i)
}
d3[29]
d3[30]
for(i in 30:length(d3)) {
download.file(url = d3[i], destfile = paste0(stri_pad_left(i, 3, 0), ".pdf"))
Sys.sleep(sample(0:10, 1))
print(i)
}
d3[60]
for(i in 60:length(d3)) {
download.file(url = d3[i], destfile = paste0(stri_pad_left(i, 3, 0), ".pdf"))
Sys.sleep(sample(0:10, 1))
print(i)
}
15*500
30*500
15*700
15*650
600*15
600*30
600*16
600*17
600*30
15*600
1600+4500
1500/2
9*500
3*3
9*500
4500/3
184366-25246
159120+25446
159120+25246
187866+117780+109580
359526+6400+4500
359526+6400+4500+7800
359526+6400+7800+4500
187866+117780+109580
159120+106080+106080+25246
396526+6400+7800+4500
29*3500
156000-101500
29*4200
156000-121800
81600*0.65
53040*3
250*3
6*47
6*43
200*280
81.02*2
2763-7
1303.79/10
1303.79/6
1303.79/10
1303.79/6
130.379/6
(130.79*7) / 6
(130.79*7) / 7
(130.79*6) / 6
(130.79*8) / 6
247/6
135+35
1303.79 / 10
130 / 6
21*5
21*6
22*6
200*12
508.04*4
338.69*6
d <- readLines("clipboard")
readLines("copdat <- read.delim("clipboard")")
readLines("clipboard")
readLines(pipe("pbpaste"))
readLines(pipe("pbpaste"))
unlist(strsplit("Wenn Fliegen neben Fliegen fliegen, fliegen Fliegen neben Fliegen"))
unlist(strsplit("Wenn Fliegen neben Fliegen fliegen, fliegen Fliegen neben Fliegen", " "))
unique(unlist(strsplit("Wenn Fliegen neben Fliegen fliegen, fliegen Fliegen neben Fliegen", " ")))
?cor.test
cor.test(c(1,2,3), c(4,5,6))
cor.test(c(1,2,3), c(4,5,6), method="kendall")
c("1548-1700", "1701-1800", "1801-1850", "1851-1900", "1901-1948")
years <- c("1548-1700", "1701-1800", "1801-1850", "1851-1900", "1901-1948")
set.seed(12345)
sample(1:3, 200, prob = c(.1,.2,.4), replace = T)
c(sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T))
c(sample(1:3, 200, prob = c(.1,.2,.8), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T),
sample(1:3, 200, prob = c(.1,.2,.4), replace = T))
c(sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.3,.5), replace = T),
sample(1:3, 200, prob = c(.1,.4,.6), replace = T),
sample(1:3, 200, prob = c(.1,.1,.7), replace = T),
sample(1:3, 200, prob = c(.1,.1,.8), replace = T))
set.seed(12345)
years <- c("1548-1700", "1701-1800", "1801-1850", "1851-1900", "1901-1948")
c(sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.3,.5), replace = T),
sample(1:3, 200, prob = c(.1,.4,.6), replace = T),
sample(1:3, 200, prob = c(.1,.1,.7), replace = T),
sample(1:3, 200, prob = c(.1,.1,.8), replace = T))
set.seed(12345)
years <- c("1548-1700", "1701-1800", "1801-1850", "1851-1900", "1901-1948")
ratings <- c(sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.3,.5), replace = T),
sample(1:3, 200, prob = c(.1,.4,.6), replace = T),
sample(1:3, 200, prob = c(.1,.1,.7), replace = T),
sample(1:3, 200, prob = c(.1,.1,.8), replace = T))
library(tidyverse)
tibble(Year = years, Rating = ratings)
rep(years[1], 200)
sapply(1:length(years), function(i) rep(years[i], 200))
unlist(sapply(1:length(years), function(i) rep(years[i], 200)))
c(unlist(sapply(1:length(years), function(i) rep(years[i], 200))))
ratings <- c(sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.3,.5), replace = T),
sample(1:3, 200, prob = c(.1,.4,.6), replace = T),
sample(1:3, 200, prob = c(.1,.1,.7), replace = T))
c(unlist(sapply(1:length(years), function(i) rep(years[i], 200))))
tibble(Year = c(unlist(sapply(1:length(years), function(i) rep(years[i], 200)))),
Rating = ratings)
d < tibble(Year = c(unlist(sapply(1:length(years), function(i) rep(years[i], 200)))),
Rating = ratings)
d <- tibble(Year = c(unlist(sapply(1:length(years), function(i) rep(years[i], 200)))),
Rating = ratings)
library(tidyverse)
set.seed(12345)
years <- c("1548-1700", "1701-1800", "1801-1850", "1851-1900", "1901-1948")
ratings <- c(sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.4,.4), replace = T),
sample(1:3, 200, prob = c(.2,.3,.5), replace = T),
sample(1:3, 200, prob = c(.1,.4,.6), replace = T),
sample(1:3, 200, prob = c(.1,.1,.7), replace = T))
d <- tibble(Year = c(unlist(sapply(1:length(years), function(i) rep(years[i], 200)))),
Rating = ratings)
d
mutate(d, is3 = ifelse(Rating == 3, "y", "n"))
# 3 or 1/2?
mutate(d, is3 = ifelse(Rating == 3, "y", "n"))
# 3 or 1/2?
mutate(d, Rating_binary = ifelse(Rating == 3, "3", "1/2"))
# 3 or 1/2?
d <- mutate(d, Rating_binary = ifelse(Rating == 3, "3", "1/2"))
d %>% group_by(Year, Rating_binary)
d %>% group_by(Year, Rating_binary) %>% summarise(
n = n()
)
18.78*4
38.9+13.1
356+203
49.3+67.2
42.55+73.30
42.55+72.30
280*12
200*12
12*80
22000-500
22000-3204
4.50*200
3515 / 105599
4293/65473
(4293/65473)*100
(3515 / 105599)*100
2061/60184
(2061/60184)*100
74+59+28+8+1+1+10
7+17+7+11+8+17+25+6+28+17+3+41+39+1+59+40+15+57
7+17+7+11+8+17+25+6+28+17+3+41+39+1+59+40+15+57+80+14+29+70+6+15+23+57+80+81+57
7+17+7+11+8+17+25+6+28+17+3+41+39+1+59+40+15+57+80+14+29+70+6+15+23+57+80+81+57+81+81
1072/60
74+59+28+8+1+1+10
88200*3
8798.19
*3
8798.19*3
33964.20/3
11321.4 + 0.5 * 11321.4
16982.1 + 0.5 * 16982.1
25473.15 + 16982.1 + 11321.4
11321.4 + 0.05 * 11321.4
11887.45 + 0.05 * 11887.45
12481.82 + 11887.45 + 11321.4
32441.82 * 0.05
32441.82 + (0.05 * 32441.82)
34063.91 + (0.05 * 34063.91)
35767.11 + 34063.91 + 32441.82
32441.82 / 3
32441.82 / 3
10813.94 + (0.05 * 10813.94)
11354.64 + (0.05 * 11354.64)
11922.37 + 11354.64 + 10813.94
26394.57 / 3
8798.19 + (0.05 * 8798.19)
9238.1 + (0.05 * 9238.1)
9700.005 + 9238.10 + 8798.19
264600 + 35690.67 + 34090.95 + 27736.29
10682.62
10682.62 + (0.05 * 10682.62)
11216.75 + (0.05 * 11216.75)
11777.59 + 11216.75 + 10682.62
264600 + 35690.67 + 34090.95 + 33676.96
16.5*3
(4.20+1.80)*3
161*12
391.03*12
12*6
0.78*113
seq(88, 160, length.out = 11)
522.15+86.70
300000/35
81600*0.65
53040/2
81600*0.75
61200/2
65.13/4
library(concordances)
citation("concordances")
6932.1/2
90*5
88200*3
8847.77*3
264600 + 26544
# Chunk 1
#| message: false
#| warning: false
# load packages
library(tidyverse)
library(tidytext)
library(ngram)
library(igraph)
library(ggraph)
library(patchwork)
# library(svglite)
# library(geomnet)
# library(rgexf)
# library(qgraph)
# library(genBaRcode)
# library(ggraph)
library(ggiraph)
library(tidygraph)
library(scales)
#install.packages(
# "microViz",
#repos = c(davidbarnett = "https://david-barnett.r-universe.dev", getOption("repos"))
#)
#if (!require("BiocManager", quietly = TRUE))
# install.packages("BiocManager")
#BiocManager::install("phyloseq")
# library(microViz)
# library(tidygraph)
# library(plotly)
library(classInt)
library(grid)
# devtools::install_github("analyxcompany/ForceAtlas2")
library(ForceAtlas2)
# library(networkD3)
# Chunk 2
testdf <- tibble(text = c("I love linguistics", "I like linguistics", "I love networks"))
test_graph <- testdf %>% unnest_tokens(output = "bigrams", input = "text", token = "ngrams", n = 2) %>% separate_wider_delim(cols = "bigrams", delim = " ", names = c("word1", "word2")) %>% graph_from_data_frame()
E(test_graph)$weight <- c(2,1,1,1,2,1)
# Chunk 3
#| include: false
set.seed(1234567)
layout <- create_layout(test_graph, layout = "fr")
layout[,1:2] <- layout.forceatlas2(test_graph, directed = TRUE, iterations = 1000) %>% as.data.frame() %>% setNames(c("x", "y"))
# Chunk 4
ggplot(layout) +
geom_edge_link(aes(x = x, y = y, xend = xend, yend = yend,
edge_width = weight), color = "gray") +
scale_edge_width(range = c(0.1, 0.7)) +
geom_node_point(col="red") +
geom_node_text(aes(label = name), position = position_nudge(y = .1)) +
scale_color_identity() + scale_fill_identity() +
theme_void() +
theme(legend.position = "none")
# ggsave("images/ilovenetworks.png", width = 7, height = 7, bg = "white")
# Chunk 5
# child data
d_fion <- read_csv("../../master/fion_CHI.csv")
140*0.6
160*0.6
setwd("'/Users/stefanhartmann/sciebo/Projekte/Minerva 2025/Let_X_Verb_Y'")
setwd('/Users/stefanhartmann/sciebo/Projekte/Minerva 2025/Let_X_Verb_Y')
library(tidyverse)
library(concordances)
library(concordances)
getCWB("data_encow/letxverby_a.txt")
d <- readLines("data_encow/letxverby_a.txt")
d
gsub(".*<|>.*", "", d)
d <- gsub(".*\\<|\\>.*", "", d)
gsub(".*\\<", "", d)
gsub(".*\<", "", d)
gsub(".*\\<", "", d)
gsub(".*<", "", d)
?iconv
iconv(d, to="utf-8")
d <- iconv(d, to="utf-8")
gsub(".*<", "", d)
gsub(".*<|>.*", "", d)
d <- gsub(".*<|>.*", "", d)
# to dataframe
unlist(strsplit(d[1], split = " "))
sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " ")))
sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[1])
library(ngram)
# wordcount
wordcount(d[1])
# wordcount
wc <- sapply(1:length(d), function(i) wordcount(d[i]))
ifelse(wc == 5, sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[5]), sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[4]))
tibble(
let = sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[1]),
noun1 = sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[2]),
verb = sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[3]),
noun2 = ifelse(wc == 5, sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[5]), sapply(1:length(d), function(i) unlist(strsplit(d[i], split = " "))[4]))
)
library(ggsankey)
# list files
f <- list.files("data_encow/", full.names = TRUE)
do.call(rbind, lappy(1:length(f), function(i) readLines(f[i])))
d <- do.call(rbind, lapply(1:length(f), function(i) readLines(f[i])))
d <- lapply(1:length(f), function(i) readLines(f[i]))
unlist(d)
d <- unlist(d)
head(d, 100)
# convert to utf8 to make sure that gsub works
d <- iconv(d, to="utf-8")
# replace anything before and after keywords
d <- gsub(".*<|>.*", "", d)
# list files, read data
f <- list.files("data_encow/", full.names = TRUE)
d <- lapply(1:length(f), function(i) readLines(f[i]))
d <- unlist(d)
# convert to utf8 to make sure that gsub works
d <- iconv(d, to="utf-8")
